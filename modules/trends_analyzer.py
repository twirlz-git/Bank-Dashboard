"""
modules/trends_analyzer.py - Search-based trends analysis through web search + LLM extraction
"""

import logging
import json
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
project_root = Path(__file__).parent.parent.absolute()
env_path = project_root / '.env'
load_dotenv(dotenv_path=env_path, override=True)

logger = logging.getLogger(__name__)


class TrendsAnalyzer:
    """Analyze product trends through web search and LLM-powered timeline extraction"""

    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None):
        """
        Initialize trends analyzer with LLM support.
        
        Args:
            api_key: API key for LLM (defaults to OPENROUTER_API_KEY)
            base_url: API base URL (defaults to OpenRouter)
        """
        self.api_key = api_key or "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"
        self.base_url = base_url or "https://openrouter.ai/api/v1"
        self.model = "tngtech/deepseek-r1t2-chimera:free"
        
        self.enabled = bool(self.api_key)
        
        if self.enabled:
            try:
                from openai import OpenAI
                self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
                
                if "openrouter" in self.base_url.lower():
                    self.client.default_headers.update({
                        "HTTP-Referer": "https://github.com/twirlz-git/Bank-Dashboard",
                        "X-Title": "Banking Product Trends Analyzer"
                    })
            except ImportError:
                logger.error("openai package not installed")
                self.enabled = False

    def analyze_trends(
        self, 
        bank: str, 
        product_type: str, 
        time_period: str,
        use_real_search: bool = False
    ) -> Dict[str, Any]:
        """
        Analyze trends for a product over time period.
        
        Args:
            bank: Bank name (e.g., "–í–¢–ë", "–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫")
            product_type: Product type (credit_card, deposit, consumer_loan)
            time_period: Time period (last_3_months, last_6_months, last_year)
            use_real_search: If True, attempt real web search (requires implementation)
        
        Returns:
            Dict with timeline, analysis, trends, and summary
        """
        
        # Calculate date range
        start_date, end_date = self._get_date_range(time_period)
        
        # Try to fetch real historical data via search
        timeline = []
        search_attempted = False
        
        if use_real_search and self.enabled:
            try:
                timeline = self._search_and_extract_timeline(
                    bank, product_type, start_date, end_date
                )
                search_attempted = True
                logger.info(f"Real search completed: {len(timeline)} data points found")
            except Exception as e:
                logger.warning(f"Real search failed: {e}, falling back to mock data")
        
        # Fallback to mock data if search not used or failed
        if not timeline:
            timeline = self._generate_mock_timeline(bank, product_type, time_period)
            logger.info(f"Using mock data: {len(timeline)} data points")
        
        # Analyze timeline
        analysis = self._analyze_timeline(timeline)
        trend_direction = self._get_trend_direction(timeline)
        
        return {
            "bank": bank,
            "product_type": product_type,
            "period": time_period,
            "start_date": start_date.strftime("%Y-%m-%d"),
            "end_date": end_date.strftime("%Y-%m-%d"),
            "timeline": timeline,
            "analysis": analysis,
            "trend_direction": trend_direction,
            "summary": self._generate_summary(bank, product_type, timeline, analysis, trend_direction),
            "data_source": "web_search" if search_attempted else "mock",
            "confidence": analysis.get("average_confidence", 0.5)
        }

    def _get_date_range(self, time_period: str) -> tuple:
        """Calculate start and end dates for given time period"""
        end_date = datetime.now()
        
        period_map = {
            "last_3_months": 92,
            "last_6_months": 180,
            "last_year": 365
        }
        
        days_back = period_map.get(time_period, 92)
        start_date = end_date - timedelta(days=days_back)
        
        return start_date, end_date

    def _search_and_extract_timeline(
        self, 
        bank: str, 
        product_type: str, 
        start_date: datetime, 
        end_date: datetime
    ) -> List[Dict[str, Any]]:
        """
        Search news sources and extract timeline using LLM.
        
        This is a placeholder for real implementation that would:
        1. Form search query with temporal filters
        2. Search through banki.ru/news, sravni.ru, kommersant.ru
        3. Collect news snippets
        4. Use LLM to extract timeline data points
        """
        
        # Build search query
        search_query = self._build_search_query(bank, product_type, start_date, end_date)
        
        # Mock news snippets (in real implementation, this would come from web search)
        mock_snippets = self._get_mock_news_snippets(bank, product_type, start_date, end_date)
        
        # Extract timeline using LLM
        timeline = self._llm_extract_timeline(
            snippets=mock_snippets,
            bank=bank,
            product_type=product_type,
            start_date=start_date,
            end_date=end_date
        )
        
        # Interpolate missing data points if needed
        timeline = self._interpolate_timeline(timeline, start_date, end_date)
        
        return timeline

    def _build_search_query(
        self, 
        bank: str, 
        product_type: str, 
        start_date: datetime, 
        end_date: datetime
    ) -> str:
        """Build search query with temporal filters"""
        
        product_names = {
            "credit_card": "–∫—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞",
            "debit_card": "–¥–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞",
            "deposit": "–≤–∫–ª–∞–¥",
            "consumer_loan": "–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–π –∫—Ä–µ–¥–∏—Ç"
        }
        
        product_name = product_names.get(product_type, product_type)
        
        query = f"–ò—Å—Ç–æ—Ä–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞–≤–æ–∫ {product_name} {bank} "
        query += f"after:{start_date.strftime('%Y-%m-%d')} "
        query += f"before:{end_date.strftime('%Y-%m-%d')} "
        query += "site:banki.ru/news OR site:sravni.ru OR site:kommersant.ru"
        
        return query

    def _get_mock_news_snippets(
        self,
        bank: str,
        product_type: str,
        start_date: datetime,
        end_date: datetime
    ) -> List[Dict[str, str]]:
        """
        Generate mock news snippets for demonstration.
        In production, this would fetch real news articles.
        """
        
        snippets = [
            {
                "date": (start_date + timedelta(days=15)).strftime("%Y-%m-%d"),
                "title": f"{bank} –∏–∑–º–µ–Ω–∏–ª —É—Å–ª–æ–≤–∏—è –ø–æ –∫—Ä–µ–¥–∏—Ç–Ω—ã–º –∫–∞—Ä—Ç–∞–º",
                "text": f"–ë–∞–Ω–∫ {bank} –ø–æ–≤—ã—Å–∏–ª –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—É—é —Å—Ç–∞–≤–∫—É –Ω–∞ 0.5% –≤ —Å–≤—è–∑–∏ —Å —Ä–µ—à–µ–Ω–∏–µ–º –¶–ë. –ù–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞ —Å–æ—Å—Ç–∞–≤–∏–ª–∞ 19.0%.",
                "source": "banki.ru"
            },
            {
                "date": (start_date + timedelta(days=45)).strftime("%Y-%m-%d"),
                "title": f"{bank}: –∏–∑–º–µ–Ω–µ–Ω–∏–µ —É—Å–ª–æ–≤–∏–π –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏—è",
                "text": f"{bank} —Å–Ω–∏–∑–∏–ª —Å—Ç–∞–≤–∫—É –¥–æ 18.5% –∏–∑-–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ –±–∞–Ω–∫–∞–º–∏.",
                "source": "sravni.ru"
            },
            {
                "date": (start_date + timedelta(days=75)).strftime("%Y-%m-%d"),
                "title": f"–ë–∞–Ω–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—Ç —Å—Ç–∞–≤–∫–∏",
                "text": f"{bank} —É—Å—Ç–∞–Ω–æ–≤–∏–ª —Å—Ç–∞–≤–∫—É –Ω–∞ —É—Ä–æ–≤–Ω–µ 17.9% –¥–ª—è –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤.",
                "source": "kommersant.ru"
            }
        ]
        
        return snippets

    def _llm_extract_timeline(
        self,
        snippets: List[Dict[str, str]],
        bank: str,
        product_type: str,
        start_date: datetime,
        end_date: datetime
    ) -> List[Dict[str, Any]]:
        """
        Use LLM to extract timeline data from news snippets.
        """
        
        if not self.enabled:
            logger.warning("LLM not available, using fallback extraction")
            return self._fallback_extract_timeline(snippets)
        
        # Build prompt for LLM
        prompt = self._build_extraction_prompt(snippets, bank, product_type)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –ò–∑–≤–ª–µ–∫–∞–π—Ç–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã –∏–∑–º–µ–Ω–µ–Ω–∏–π –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏–π –∏–∑ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π. –û—Ç–≤–µ—á–∞–π—Ç–µ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            
            result = json.loads(response.choices[0].message.content)
            timeline = result.get("timeline", [])
            
            logger.info(f"LLM extracted {len(timeline)} data points")
            return timeline
            
        except Exception as e:
            logger.error(f"LLM extraction failed: {e}")
            return self._fallback_extract_timeline(snippets)

    def _build_extraction_prompt(
        self,
        snippets: List[Dict[str, str]],
        bank: str,
        product_type: str
    ) -> str:
        """Build prompt for LLM timeline extraction"""
        
        snippets_text = "\n\n".join([
            f"–î–∞—Ç–∞: {s['date']}\n–ò—Å—Ç–æ—á–Ω–∏–∫: {s['source']}\n–ó–∞–≥–æ–ª–æ–≤–æ–∫: {s['title']}\n–¢–µ–∫—Å—Ç: {s['text']}"
            for s in snippets
        ])
        
        product_names = {
            "credit_card": "–∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∫–∞—Ä—Ç–µ",
            "deposit": "–≤–∫–ª–∞–¥—É",
            "consumer_loan": "–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–æ–º—É –∫—Ä–µ–¥–∏—Ç—É"
        }
        product_name = product_names.get(product_type, "–±–∞–Ω–∫–æ–≤—Å–∫–æ–º—É –ø—Ä–æ–¥—É–∫—Ç—É")
        
        return f"""–ò–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏–∑–≤–ª–µ–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–π —Å—Ç–∞–≤–∫–∏ –ø–æ {product_name} –±–∞–Ω–∫–∞ {bank}.

–ù–û–í–û–°–¢–ò:
{snippets_text}

–ó–ê–î–ê–ß–ê:
–ò–∑–≤–ª–µ–∫–∏ –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π —Ç–æ—á–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞–≤–∫–∏ –∏ –≤–µ—Ä–Ω–∏ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ JSON:

{{
  "timeline": [
    {{
      "date": "YYYY-MM-DD",
      "rate": 18.5,
      "reason": "–†–µ—à–µ–Ω–∏–µ –¶–ë / –ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è / –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥ / –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è",
      "confidence": 0.9,
      "source": "banki.ru"
    }}
  ]
}}

–ü–†–ê–í–ò–õ–ê:
- –ï—Å–ª–∏ –¥–∞—Ç–∞ —Ç–æ—á–Ω–∞—è - –∏—Å–ø–æ–ª—å–∑—É–π –µ—ë
- –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ—Ç–æ—á–Ω–∞—è ("–≤ –Ω–∞—á–∞–ª–µ –º–µ—Å—è—Ü–∞") - –∏—Å–ø–æ–ª—å–∑—É–π 15 —á–∏—Å–ª–æ
- –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–π –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏
- confidence: 0.9 –¥–ª—è —Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, 0.7 –¥–ª—è –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã—Ö, 0.5 –¥–ª—è –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - –≤–æ–∑–≤—Ä–∞—â–∞–π –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤
- –ò–∑–≤–ª–µ–∫–∞–π –¢–û–õ–¨–ö–û —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö —Å—Ç–∞–≤–æ–∫ {bank}
"""

    def _fallback_extract_timeline(self, snippets: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """Fallback extraction without LLM (simple regex-based)"""
        
        import re
        timeline = []
        
        for snippet in snippets:
            # Try to extract rate from text using regex
            rate_match = re.search(r'(\d+[.,]\d+)%', snippet['text'])
            
            if rate_match:
                rate_str = rate_match.group(1).replace(',', '.')
                try:
                    rate = float(rate_str)
                    timeline.append({
                        "date": snippet['date'],
                        "rate": rate,
                        "reason": "–ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏",
                        "confidence": 0.6,
                        "source": snippet.get('source', 'unknown')
                    })
                except ValueError:
                    pass
        
        return timeline

    def _interpolate_timeline(
        self,
        timeline: List[Dict[str, Any]],
        start_date: datetime,
        end_date: datetime
    ) -> List[Dict[str, Any]]:
        """
        Interpolate missing data points in timeline.
        If there are gaps between data points, fill with linear interpolation.
        """
        
        if len(timeline) < 2:
            return timeline
        
        # Sort by date
        timeline = sorted(timeline, key=lambda x: x['date'])
        
        # For MVP, return as-is (full interpolation implementation can be added later)
        return timeline

    def _generate_mock_timeline(
        self, 
        bank: str, 
        product_type: str, 
        time_period: str
    ) -> List[Dict[str, Any]]:
        """Generate mock timeline for demonstration"""
        
        base_date = datetime.now()
        timeline = []
        
        if time_period == "last_3_months":
            days_back = 92
            intervals = [0, 30, 60, 90]
        elif time_period == "last_6_months":
            days_back = 180
            intervals = [0, 45, 90, 135, 180]
        else:
            days_back = 365
            intervals = [0, 90, 180, 270, 360]
        
        # Mock data patterns
        base_rate = 19.5
        rates = [base_rate - (i * 0.4) for i in range(len(intervals))]
        reasons = [
            "–ù–∞—á–∞–ª–æ –ø–µ—Ä–∏–æ–¥–∞",
            "–†–µ—à–µ–Ω–∏–µ –¶–ë",
            "–†–µ–∞–∫—Ü–∏—è –Ω–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—é",
            "–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–∞—è –∞–∫—Ü–∏—è",
            "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è"
        ]
        
        for idx, days_ago in enumerate(reversed(intervals)):
            date = base_date - timedelta(days=days_ago)
            timeline.append({
                "date": date.strftime("%Y-%m-%d"),
                "rate": rates[idx % len(rates)],
                "reason": reasons[idx % len(reasons)],
                "confidence": 0.75,
                "source": "mock"
            })
        
        return timeline

    def _analyze_timeline(self, timeline: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze timeline for patterns and statistics"""
        
        if not timeline:
            return {"status": "no_data"}
        
        rates = [t.get("rate") for t in timeline if t.get("rate") is not None]
        
        if len(rates) < 2:
            return {"status": "insufficient_data"}
        
        confidences = [t.get("confidence", 0.5) for t in timeline]
        
        return {
            "status": "success",
            "start_value": rates[0],
            "end_value": rates[-1],
            "min_value": min(rates),
            "max_value": max(rates),
            "average_value": sum(rates) / len(rates),
            "total_change": rates[-1] - rates[0],
            "change_percentage": ((rates[-1] - rates[0]) / rates[0] * 100) if rates[0] != 0 else 0,
            "data_points": len(rates),
            "change_points": self._count_change_points(rates),
            "average_confidence": sum(confidences) / len(confidences)
        }

    def _count_change_points(self, values: List[float], threshold: float = 0.1) -> int:
        """Count significant change points in values"""
        
        if len(values) < 2:
            return 0
        
        changes = 0
        for i in range(1, len(values)):
            if abs(values[i] - values[i-1]) >= threshold:
                changes += 1
        
        return changes

    def _get_trend_direction(self, timeline: List[Dict[str, Any]]) -> str:
        """Determine overall trend direction"""
        
        if not timeline or len(timeline) < 2:
            return "stable"
        
        rates = [t.get("rate") for t in timeline if t.get("rate") is not None]
        
        if len(rates) < 2:
            return "stable"
        
        start = rates[0]
        end = rates[-1]
        
        change_pct = ((end - start) / start * 100) if start != 0 else 0
        
        if change_pct > 2:
            return "increasing"
        elif change_pct < -2:
            return "decreasing"
        else:
            return "stable"

    def _generate_summary(
        self, 
        bank: str, 
        product_type: str, 
        timeline: List[Dict[str, Any]],
        analysis: Dict[str, Any],
        trend: str
    ) -> str:
        """Generate comprehensive human-readable summary"""
        
        if analysis.get("status") == "no_data":
            return f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ {product_type} –±–∞–Ω–∫–∞ {bank}"
        
        if analysis.get("status") == "insufficient_data":
            return f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ {product_type} –±–∞–Ω–∫–∞ {bank}"
        
        product_names = {
            "credit_card": "–∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∫–∞—Ä—Ç—ã",
            "debit_card": "–¥–µ–±–µ—Ç–æ–≤–æ–π –∫–∞—Ä—Ç—ã",
            "deposit": "–≤–∫–ª–∞–¥–∞",
            "consumer_loan": "–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫—Ä–µ–¥–∏—Ç–∞"
        }
        product_name = product_names.get(product_type, product_type)
        
        trend_icons = {
            "increasing": "üìà –†–∞—Å—Ç—É—â–∏–π",
            "decreasing": "üìâ –ü–∞–¥–∞—é—â–∏–π",
            "stable": "‚Üí –°—Ç–∞–±–∏–ª—å–Ω—ã–π"
        }
        trend_text = trend_icons.get(trend, "‚Üí –°—Ç–∞–±–∏–ª—å–Ω—ã–π")
        
        summary = f"**–ê–Ω–∞–ª–∏–∑ {product_name} –±–∞–Ω–∫–∞ {bank}**\n\n"
        summary += f"‚Ä¢ **–ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:** {analysis.get('start_value', 0):.2f}%\n"
        summary += f"‚Ä¢ **–ö–æ–Ω–µ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:** {analysis.get('end_value', 0):.2f}%\n"
        summary += f"‚Ä¢ **–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:** {analysis.get('average_value', 0):.2f}%\n"
        summary += f"‚Ä¢ **–î–∏–∞–ø–∞–∑–æ–Ω:** {analysis.get('min_value', 0):.2f}% - {analysis.get('max_value', 0):.2f}%\n"
        summary += f"‚Ä¢ **–¢—Ä–µ–Ω–¥:** {trend_text}\n"
        
        change = analysis.get('total_change', 0)
        change_pct = analysis.get('change_percentage', 0)
        
        if change != 0:
            summary += f"‚Ä¢ **–ò–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞ –ø–µ—Ä–∏–æ–¥:** {change:+.2f}% ({change_pct:+.1f}%)\n"
        
        change_points = analysis.get('change_points', 0)
        summary += f"‚Ä¢ **–¢–æ—á–µ–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è:** {change_points}\n"
        
        confidence = analysis.get('average_confidence', 0)
        summary += f"‚Ä¢ **–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö:** {confidence:.0%}\n"
        
        # Add interpretation
        summary += f"\n**üí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:**\n"
        
        if trend == "decreasing":
            summary += "–°—Ç–∞–≤–∫–∞ —Å–Ω–∏–∂–∞–µ—Ç—Å—è - –≤–æ–∑–º–æ–∂–Ω–æ, –±–∞–Ω–∫ —Å—Ç—Ä–µ–º–∏—Ç—Å—è –ø—Ä–∏–≤–ª–µ—á—å –±–æ–ª—å—à–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏–ª–∏ —Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—é.\n"
        elif trend == "increasing":
            summary += "–°—Ç–∞–≤–∫–∞ —Ä–∞—Å—Ç—ë—Ç - –≤–æ–∑–º–æ–∂–Ω–æ, –±–∞–Ω–∫ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç —É—Å–ª–æ–≤–∏—è –≤ —Å–≤—è–∑–∏ —Å —Ä–µ—à–µ–Ω–∏—è–º–∏ –¶–ë –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º —Ä—ã–Ω–æ—á–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏.\n"
        else:
            summary += "–°—Ç–∞–≤–∫–∞ –æ—Å—Ç–∞—ë—Ç—Å—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π - —É—Å–ª–æ–≤–∏—è –Ω–µ –º–µ–Ω—è–ª–∏—Å—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤ —Ç–µ—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞.\n"
        
        return summary


